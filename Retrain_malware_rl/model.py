import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init
import numpy as np

class Net_ppo_parallel(nn.Module):
    def __init__(self):
        super(Net_ppo_parallel, self).__init__()
        self.input_length=1048576
        self.window_size=500
        self.embed = nn.Embedding(257, 8, padding_idx=0)

        self.conv_1 = nn.Conv1d(4, 128, self.window_size, stride=self.window_size, bias=True)
        self.conv_2 = nn.Conv1d(4, 128, self.window_size, stride=self.window_size, bias=True)

        self.pooling = nn.MaxPool1d(int(self.input_length/self.window_size))
        

        self.fc_1 = nn.Linear(128,128)
        # self.fc_2 = nn.Linear(128,action_shape)

        self.sigmoid = nn.Sigmoid()
        #self.softmax = nn.Softmax()
        self.output_dim = 128

    def forward(self, obs, state=None, info={}):
        obs = obs.int() # data parallel needs this
        # obs = obs.astype(int)
        # obs = torch.from_numpy(obs).cuda() # without data parallel needs this
        x = self.embed(obs)
        # Channel first
        x = torch.transpose(x,-1,-2)

        cnn_value = self.conv_1(x.narrow(-2, 0, 4))
        gating_weight = self.sigmoid(self.conv_2(x.narrow(-2, 4, 4)))

        x = cnn_value * gating_weight
        x = self.pooling(x)

        x = x.view(-1,128)
        x = self.fc_1(x)
        # x = self.fc_2(x)
        #x = self.sigmoid(x)

        return x, state

class Net_ppo(nn.Module):
    def __init__(self):
        super(Net_ppo, self).__init__()
        self.input_length=1048576
        self.window_size=500
        self.embed = nn.Embedding(257, 8, padding_idx=0)

        self.conv_1 = nn.Conv1d(4, 128, self.window_size, stride=self.window_size, bias=True)
        self.conv_2 = nn.Conv1d(4, 128, self.window_size, stride=self.window_size, bias=True)

        self.pooling = nn.MaxPool1d(int(self.input_length/self.window_size))
        

        self.fc_1 = nn.Linear(128,128)
        # self.fc_2 = nn.Linear(128,action_shape)

        self.sigmoid = nn.Sigmoid()
        #self.softmax = nn.Softmax()
        self.output_dim = 128

    def forward(self, obs, state=None, info={}):
        # obs = obs.int() # data parallel needs this
        obs = obs.astype(int)
        obs = torch.from_numpy(obs).cuda() # without data parallel needs this

        x = self.embed(obs)
        # Channel first
        x = torch.transpose(x,-1,-2)

        cnn_value = self.conv_1(x.narrow(-2, 0, 4))
        gating_weight = self.sigmoid(self.conv_2(x.narrow(-2, 4, 4)))

        x = cnn_value * gating_weight
        x = self.pooling(x)

        x = x.view(-1,128)
        x = self.fc_1(x)
        # x = self.fc_2(x)
        #x = self.sigmoid(x)

        return x, state

class Net_dqn(nn.Module):
    def __init__(self, action_shape, device):
        super(Net_dqn, self).__init__()
        self.input_length=1048576
        self.window_size=500
        self.embed = nn.Embedding(257, 8, padding_idx=0)

        self.conv_1 = nn.Conv1d(4, 128, self.window_size, stride=self.window_size, bias=True)
        self.conv_2 = nn.Conv1d(4, 128, self.window_size, stride=self.window_size, bias=True)

        self.pooling = nn.MaxPool1d(int(self.input_length/self.window_size))
        

        self.fc_1 = nn.Linear(128,128)
        self.fc_2 = nn.Linear(128,action_shape)

        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax()
        # self.output_dim = 128
        self.device = device

    def forward(self, obs, state=None, info={}):
        obs = obs.astype(int)
        obs = torch.from_numpy(obs).to(self.device)
        x = self.embed(obs)
        # Channel first
        x = torch.transpose(x,-1,-2)

        cnn_value = self.conv_1(x.narrow(-2, 0, 4))
        gating_weight = self.sigmoid(self.conv_2(x.narrow(-2, 4, 4)))

        x = cnn_value * gating_weight
        x = self.pooling(x)

        x = x.view(-1,128)
        x = self.fc_1(x)
        x = self.fc_2(x)
        x = self.sigmoid(x)

        return x, state

class RNDModel(nn.Module):
    def __init__(self, output_dim, device):
        super(RNDModel, self).__init__()
        self.input_length = 1048576
        self.window_size = 500

        self.embed = nn.Embedding(257, 8, padding_idx=0)

        self.conv_1 = nn.Conv1d(4, 128, self.window_size, stride=self.window_size, bias=True)
        self.conv_2 = nn.Conv1d(4, 128, self.window_size, stride=self.window_size, bias=True)

        self.pooling = nn.MaxPool1d(int(self.input_length / self.window_size))

        self.fc_1 = nn.Linear(128, 128)
        self.fc_2 = nn.Linear(128, output_dim)

        self.sigmoid = nn.Sigmoid()
        self.softmax = nn.Softmax()
        self.device = device

        self.predictor = nn.Sequential(
            self.embed,
            nn.Transpose(1, 2),  # Channel first
            self.conv_1,
            self.sigmoid,  # Applying sigmoid to the output of conv_2
            nn.Conv1d(128, 128, 1),  # Adding a 1x1 convolution for element-wise multiplication
            nn.MaxPool1d(int(self.input_length / self.window_size)),
            nn.Flatten(),
            self.fc_1,
            self.fc_2,
            self.sigmoid
        )

        self.target = nn.Sequential(
            self.embed,
            nn.Transpose(1, 2),  # Channel first
            self.conv_1,
            self.sigmoid,  # Applying sigmoid to the output of conv_2
            nn.Conv1d(128, 128, 1),  # Adding a 1x1 convolution for element-wise multiplication
            nn.MaxPool1d(int(self.input_length / self.window_size)),
            nn.Flatten(),
            self.fc_1,
            self.fc_2,
            self.sigmoid
        )

        for m in self.modules():
            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):
                init.orthogonal_(m.weight, np.sqrt(2))
                m.bias.data.zero_()

        for param in self.target.parameters():
            param.requires_grad = False

    def forward(self, obs):
        target_feature = self.target(obs)
        predict_feature = self.predictor(obs)
        forward_loss = F.mse_loss(predict_feature, target_feature.detach())
        return forward_loss
    
    def compute_bonus(self, next_obs):
        next_obs = torch.FloatTensor(next_obs).to(self.device)
        target_next_feature = self.target(next_obs)
        predict_next_feature = self.predictor(next_obs)
        intrinsic_reward = (target_next_feature - predict_next_feature).pow(2).sum(1) / 2
        return intrinsic_reward.data.cpu().numpy()