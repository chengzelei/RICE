import hashlib
import os
import random
import sys
from collections import OrderedDict
import numpy as np
import gym
import torch
from gym import spaces
from model import Net_ppo
from malware_rl.envs.controls import modifier
from malware_rl.envs.utils import interface, malconv
from tianshou.data import Batch
from malware_rl.envs.load_models import load_baseline_model, load_masknet
import argparse

module_path = os.path.split(os.path.abspath(sys.modules[__name__].__file__))[0]

ACTION_LOOKUP = {i: act for i, act in enumerate(modifier.ACTION_TABLE.keys())}

mc = malconv.MalConv()
malicious_threshold = mc.malicious_threshold

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--task', type=str, default="malconv-train-v0")
    parser.add_argument('--reward-threshold', type=float, default=None)
    parser.add_argument('--seed', type=int, default=1626)
    parser.add_argument('--buffer-size', type=int, default=20000)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--epoch', type=int, default=500)
    parser.add_argument('--step-per-epoch', type=int, default=2000)
    parser.add_argument('--episode-per-collect', type=int, default=100)
    parser.add_argument('--repeat-per-collect', type=int, default=2)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[1024, 512, 256])
    parser.add_argument('--training-num', type=int, default=80)
    parser.add_argument('--test-num', type=int, default=20)
    parser.add_argument('--logdir', type=str, default='log')
    parser.add_argument('--render', type=float, default=0.)
    parser.add_argument(
        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'
    )
    # ppo special
    parser.add_argument('--vf-coef', type=float, default=0.5)
    parser.add_argument('--ent-coef', type=float, default=0.0)
    parser.add_argument('--eps-clip', type=float, default=0.2)
    parser.add_argument('--max-grad-norm', type=float, default=0.5)
    parser.add_argument('--gae-lambda', type=float, default=0.95)
    parser.add_argument('--rew-norm', type=int, default=0)
    parser.add_argument('--norm-adv', type=int, default=0)
    parser.add_argument('--recompute-adv', type=int, default=0)
    parser.add_argument('--dual-clip', type=float, default=None)
    parser.add_argument('--value-clip', type=int, default=0)
    args = parser.parse_known_args()[0]
    return args



class MalConvEnv_retrain(gym.Env):
    """Create MalConv gym interface"""

    metadata = {"render.modes": ["human"]}

    def __init__(
        self,
        sha256list,
        go_prob=0.5,
        wo_exp=False,
        random_sample=False,
        maxturns=5,
        output_path="data/evaded/malconv",
    ):
        super().__init__()
        self.available_sha256 = sha256list
        self.action_space = spaces.Discrete(len(ACTION_LOOKUP))
        self.observation_space = spaces.Box(
            low=0,
            high=256,
            shape=(1048576,),
            dtype=np.int16,
        )
        self.maxturns = maxturns
        self.go_prob = go_prob
        self.wo_exp = wo_exp
        self.feature_extractor = mc.extract
        self.output_path = output_path
        self.random_sample = random_sample
        self.history = OrderedDict()
        self.sample_iteration_index = 0
        self.rollout_env = gym.make("malconv-train-v0", random_sample=False)

        self.output_path = os.path.join(
            os.path.dirname(
                os.path.dirname(
                    os.path.dirname(
                        os.path.abspath(__file__),
                    ),
                ),
            ),
            output_path,
        )

    def step(self, action_ix):
        # Execute one time step within the environment
        self.turns += 1
        self._take_action(action_ix)
        self.observation_space = self.feature_extractor(self.bytez)
        self.score = mc.predict_sample(self.observation_space)

        if self.score < malicious_threshold:
            reward = 10.0
            episode_over = True
            self.history[self.sha256]["evaded"] = True
            self.history[self.sha256]["reward"] = reward

            # save off file to evasion directory
            m = hashlib.sha256()
            m.update(self.bytez)
            sha256 = m.hexdigest()
            evade_path = os.path.join(self.output_path, sha256)

            # with open(evade_path, "wb") as out:
            #     out.write(self.bytez)

            self.history[self.sha256]["evade_path"] = evade_path

        elif self.turns >= self.maxturns:
            # game over - max turns hit
            reward = self.original_score - self.score
            episode_over = True
            self.history[self.sha256]["evaded"] = False
            self.history[self.sha256]["reward"] = reward

        else:
            reward = float(self.original_score - self.score)
            episode_over = False

        if episode_over:
            # print(f"Episode over: reward = {reward}")
            pass

        return self.observation_space, reward, episode_over, self.history[self.sha256]

    def _take_action(self, action_ix):
        action = ACTION_LOOKUP[int(action_ix)]
        # print("SHA:{}, trial:{}, action:{}".format(self.sha256, self.turns, ACTION_LOOKUP[int(action_ix)]))
        try:
            self.bytez = modifier.modify_sample(self.bytez, action)
            # self.bytez = bytes(modifier.modify_without_breaking(self.bytez, [action]))
        except:
            # print("Error in action:", action)
            pass
            

    def reset(self):
        # Reset the state of the environment to an initial state
        self.turns = 0
        while True:
            # grab a new sample (TODO)
            if self.random_sample:
                self.sha256 = np.random.choice(self.available_sha256)
            else:
                self.sha256 = self.available_sha256[
                    self.sample_iteration_index % len(self.available_sha256)
                ]
                self.sample_iteration_index += 1

            self.history[self.sha256] = {"evaded": False}
            self.bytez = interface.fetch_file(
                os.path.join(
                    module_path,
                    "utils/samples/",
                )
                + self.sha256,
            )

            self.observation_space = self.feature_extractor(self.bytez)
            
            action_record, mask_record = self.gen_one_traj()
            if np.random.rand() < self.go_prob:
                if self.wo_exp:
                    start_idx = np.random.choice(len(action_record))
                else:
                    start_idx = np.argmax(mask_record)

                for k in range(start_idx):
                    self._take_action(action_record[k])
                self.observation_space = self.feature_extractor(self.bytez)

            self.original_score = mc.predict_sample(self.observation_space)

            if self.original_score < malicious_threshold:
                # already labeled benign, skip
                continue
            break
        

        # print(f"Sample: {self.sha256}")

        return self.observation_space

    def render(self, mode="human", close=False):
        # Render the environment to the screen
        pass

    def seed(self, seed=None):
        # Set the seed for this env's random number generator(s)
        random.seed(seed)

    def calculate_q(self, action):
        self._take_action(action)
        self.observation_space = self.feature_extractor(self.bytez)
        self.score = mc.predict_sample(self.observation_space)
        if self.score < malicious_threshold:
            reward = 10.0
        else:
            reward = float(self.original_score - self.score)
        return self.observation_space, reward

    def gen_one_traj(self):
        args = get_args()
        baseline_model = load_baseline_model()
        mask_model = load_masknet()
        mask_record = []
        action_record = []
        obs = self.rollout_env.reset()
        done = False

        while not done:
            batch = Batch(obs=obs.reshape((1,-1)), info=None)
            action = baseline_model(batch).act[0]

            mask_prob = mask_model(batch).logits[0][0] #the prob of not doing the mask-> state importance
            mask_record.append(mask_prob.detach().cpu())
            action_record.append(action)

            obs, reward, done, _ = self.rollout_env.step(action)
        
        return action_record, mask_record




    
