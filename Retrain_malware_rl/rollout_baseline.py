import os
import random
import sys

import gym
import numpy as np
from gym import wrappers
from IPython import embed
import time
import malware_rl
import malware_rl.envs
from stable_baselines3 import PPO
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from model import Net_ppo, Net_ppo_parallel
from tianshou.data import AsyncCollector, VectorReplayBuffer, Collector
from tianshou.env import DummyVectorEnv
from tianshou.policy import PPOPolicy
from tianshou.trainer import onpolicy_trainer
from tianshou.utils import TensorboardLogger
from tianshou.utils.net.common import ActorCritic, DataParallelNet
from tianshou.utils.net.discrete import Actor, Critic
import argparse
import torch
from tianshou.data import Batch
module_path = os.path.split(os.path.abspath(sys.modules[__name__].__file__))[0]
from sample_mask_score import set_seed
from model import Net_dqn
from tianshou.policy import DQNPolicy
def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--task', type=str, default="malconv-train-v0")
    parser.add_argument('--reward-threshold', type=float, default=None)
    parser.add_argument('--seed', type=int, default=1626)
    parser.add_argument('--buffer-size', type=int, default=20000)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--epoch', type=int, default=500)
    parser.add_argument('--n-step', type=int, default=2)
    parser.add_argument('--target-update-freq', type=int, default=320)
    parser.add_argument('--step-per-epoch', type=int, default=2000)
    parser.add_argument('--episode-per-collect', type=int, default=100)
    parser.add_argument('--repeat-per-collect', type=int, default=2)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[1024, 512, 256])
    parser.add_argument('--training-num', type=int, default=80)
    parser.add_argument('--test-num', type=int, default=20)
    parser.add_argument('--logdir', type=str, default='log')
    parser.add_argument('--render', type=float, default=0.)
    parser.add_argument(
        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'
    )
    # ppo special
    parser.add_argument('--vf-coef', type=float, default=0.5)
    parser.add_argument('--ent-coef', type=float, default=0.0)
    parser.add_argument('--eps-clip', type=float, default=0.2)
    parser.add_argument('--max-grad-norm', type=float, default=0.5)
    parser.add_argument('--gae-lambda', type=float, default=0.95)
    parser.add_argument('--rew-norm', type=int, default=0)
    parser.add_argument('--norm-adv', type=int, default=0)
    parser.add_argument('--recompute-adv', type=int, default=0)
    parser.add_argument('--dual-clip', type=float, default=None)
    parser.add_argument('--value-clip', type=int, default=0)
    args = parser.parse_known_args()[0]
    return args

start = time.time()
args=get_args()
outdir = os.path.join(module_path, "data/logs/random-agent-results")
env = gym.make("malware_rl.envs:malconv-train-v0")
args.state_space = env.observation_space.shape or env.observation_space.n
args.action_shape = env.action_space.shape or env.action_space.n
if args.reward_threshold is None:
    default_reward_threshold = {"malconv-train-v0": 10}
    args.reward_threshold = default_reward_threshold.get(
        args.task, env.spec.reward_threshold
    )

# seed
np.random.seed(args.seed)
torch.manual_seed(args.seed)
# ppo_model

# net = Net_ppo_parallel().to(args.device)
# if torch.cuda.is_available():
#     actor = DataParallelNet(
#         Actor(net, args.action_shape, device=None).to(args.device)
#     )
#     critic = DataParallelNet(Critic(net, device=None).to(args.device))
# else:
#     actor = Actor(net, args.action_shape, device=args.device).to(args.device)
#     critic = Critic(net, device=args.device).to(args.device)
# actor_critic = ActorCritic(actor, critic)
# # orthogonal initialization
# for m in actor_critic.modules():
#     if isinstance(m, torch.nn.Linear):
#         torch.nn.init.orthogonal_(m.weight)
#         torch.nn.init.zeros_(m.bias)
# optim = torch.optim.Adam(actor_critic.parameters(), lr=args.lr)
# dist = torch.distributions.Categorical
# policy = PPOPolicy(
#     actor,
#     critic,
#     optim,
#     dist,
#     discount_factor=args.gamma,
#     max_grad_norm=args.max_grad_norm,
#     eps_clip=args.eps_clip,
#     vf_coef=args.vf_coef,
#     ent_coef=args.ent_coef,
#     gae_lambda=args.gae_lambda,
#     reward_normalization=args.rew_norm,
#     dual_clip=args.dual_clip,
#     value_clip=args.value_clip,
#     action_space=env.action_space,
#     deterministic_eval=True,
#     advantage_normalization=args.norm_adv,
#     recompute_advantage=args.recompute_adv
# )

# # log
# log_path = os.path.join(args.logdir, args.task, 'ppo')
# # policy.load_state_dict(torch.load('/data/jiahao/malware_rl/logs/checkpoint.pth')["model"])
# policy.load_state_dict(torch.load('log/malconv-train-v0/ppo/checkpoint.pth')["model"])

net = Net_dqn(args.action_shape,args.device).to(args.device)
optim = torch.optim.Adam(net.parameters(), lr=args.lr)
policy = DQNPolicy(
        net,
        optim,
        args.gamma,
        args.n_step,
        target_update_freq=args.target_update_freq,
    )
# policy.load_state_dict(torch.load('models/dqn_baseline_ckpt1.pth')["model"])
policy.load_state_dict(torch.load('/home/zck7060/malware_rl/retrain_models/malconv-retrain-v0/go_prob_0.5_bonus_scale_0.01_exp/best_policy.pth')["model"])
env.seed(0)
episode_count = 500
done = False
reward = 0

# metric tracking
evasions = 0
evasion_history = {}


for i in range(episode_count):
    ob = env.reset()
    while True:
        batch = Batch(obs=ob.reshape((1,-1)), info=None)
        action = policy(batch).act[0]
        ob, reward, done, ep_history = env.step(action)
        if done and reward >= 10.0:
            evasions += 1
            break

        elif done:
            break

evasion_rate = (evasions / episode_count) * 100
# mean_action_count = np.mean(env.get_episode_lengths())
print(f"{evasion_rate}% samples evaded model.")
# print(f"Average of {mean_action_count} moves to evade model.")
end = time.time()
print(f"Time elapsed: {end - start}")
