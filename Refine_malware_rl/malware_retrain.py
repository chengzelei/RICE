import argparse
import os
import pprint
from rnd_policy import RNDPolicy
import gym
import numpy as np
import torch
from torch.utils.tensorboard import SummaryWriter
from tianshou.data import Collector, PrioritizedVectorReplayBuffer, VectorReplayBuffer
from tianshou.env import DummyVectorEnv
from tianshou.policy import DQNPolicy
from tianshou.trainer import offpolicy_trainer
from tianshou.utils import TensorboardLogger
from tianshou.utils.net.common import ActorCritic, DataParallelNet
from model import Net_dqn, RNDModel
from tianshou.utils.net.discrete import Actor, Critic
import malware_rl
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'


def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--task', type=str, default='malconv-retrain-v0')
    parser.add_argument('--test_task', type=str, default='malconv-train-v0')
    parser.add_argument('--reward-threshold', type=float, default=15)
    parser.add_argument('--seed', type=int, default=1626)
    parser.add_argument('--eps-test', type=float, default=0.0)
    parser.add_argument('--eps-train', type=float, default=0.05)
    parser.add_argument('--buffer-size', type=int, default=20000)
    parser.add_argument('--lr', type=float, default=1e-5)
    parser.add_argument('--gamma', type=float, default=0.99)
    parser.add_argument('--n-step', type=int, default=2)
    parser.add_argument('--target-update-freq', type=int, default=320)
    parser.add_argument('--epoch', type=int, default=100)
    parser.add_argument('--step-per-epoch', type=int, default=2000)
    parser.add_argument('--step-per-collect', type=int, default=1000)
    parser.add_argument('--repeat-per-collect', type=int, default=2)
    parser.add_argument('--batch-size', type=int, default=64)
    parser.add_argument('--hidden-sizes', type=int, nargs='*', default=[1024, 512, 256])
    parser.add_argument('--update-per-step', type=float, default=0.1)
    parser.add_argument('--training-num', type=int, default=80)
    parser.add_argument('--test-num', type=int, default=10)
    parser.add_argument('--logdir', type=str, default='retrain_models')
    parser.add_argument('--render', type=float, default=0.)
    parser.add_argument(
        '--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu'
    )
    parser.add_argument('--prioritized-replay', action="store_true", default=False)
    parser.add_argument('--alpha', type=float, default=0.6)
    parser.add_argument('--beta', type=float, default=0.4)
    parser.add_argument('--rnd-lr', type=float, default=1e-5)
    parser.add_argument('--rnd-bonus-scale', type=float, default=1e-2)
    parser.add_argument('--rnd-loss-weight', type=float, default=1)
    parser.add_argument('--go-prob', type=float, default=0.5)
    parser.add_argument('--wo_exp', default=False)

    args = parser.parse_known_args()[0]
    return args


def retrain_dqn(args=get_args()):
    env = gym.make(args.task)
    args.state_shape = env.observation_space.shape or env.observation_space.n
    args.action_shape = env.action_space.shape or env.action_space.n
    if args.reward_threshold is None:
        default_reward_threshold = {"malconv-train-v0": 10}
        args.reward_threshold = default_reward_threshold.get(
            args.task, env.spec.reward_threshold
        )

    # you can also use tianshou.env.SubprocVectorEnv
    train_envs = DummyVectorEnv(
        [lambda: gym.make(args.task, go_prob=args.go_prob,\
        wo_exp=args.wo_exp) for _ in range(args.training_num)]
    )
   
    test_envs = DummyVectorEnv(
        [lambda: gym.make(args.test_task) for _ in range(args.test_num)]
    )
    # seed
    np.random.seed(args.seed)
    torch.manual_seed(args.seed)
    train_envs.seed(args.seed)
    test_envs.seed(args.seed)
    # model
    # net = Net(args.state_shape, hidden_sizes=args.hidden_sizes, device=args.device)
    net = Net_dqn(args.action_shape,args.device).to(args.device)
    optim = torch.optim.Adam(net.parameters(), lr=args.lr)
    policy = DQNPolicy(
        net,
        optim,
        args.gamma,
        args.n_step,
        target_update_freq=args.target_update_freq,
    )
    policy.load_state_dict(torch.load('/home/zck7060/malware_rl/models/dqn_baseline_ckpt1.pth')["model"])
    
    rnd_model = RNDModel(64,args.device).to(args.device)
    rnd_optim = torch.optim.Adam(rnd_model.parameters(), lr=args.rnd_lr)
    policy = RNDPolicy(
            policy, rnd_model, rnd_optim, args.rnd_bonus_scale, args.rnd_loss_weight
        ).to(args.device)
    
    
    # buffer
    if args.prioritized_replay:
        buf = PrioritizedVectorReplayBuffer(
            args.buffer_size,
            buffer_num=len(train_envs),
            alpha=args.alpha,
            beta=args.beta,
        )
    else:
        buf = VectorReplayBuffer(args.buffer_size, buffer_num=len(train_envs))
    # collector
    train_collector = Collector(policy, train_envs, buf, exploration_noise=True)
    test_collector = Collector(policy, test_envs, exploration_noise=False)
    # test_collector = None
    # log
    param_str = 'go_prob_' + str(args.go_prob) + '_bonus_scale_' + str(args.rnd_bonus_scale)
    if args.wo_exp:
        param_str += '_no_exp'
    else:
        param_str += '_exp'
    log_path = os.path.join(args.logdir, args.task, param_str, str(args.seed))
    if not os.path.exists(log_path):
        os.makedirs(log_path)
    writer = SummaryWriter(log_path)
    logger = TensorboardLogger(writer)

    def save_best_fn(policy):
        torch.save(                
            {
                    "model": policy.state_dict(),
                    "optim": optim.state_dict(),
                }, os.path.join(log_path, 'best_policy.pth'))

    def save_checkpoint_fn(epoch, env_step, gradient_step):

        # ckpt_path = os.path.join(log_path, "checkpoint.pth")
        # Example: saving by epoch num
        ckpt_path = os.path.join(log_path, f"checkpoint_{epoch}.pth")
        if epoch % 5 == 0:
            torch.save(
                {
                    "model": policy.state_dict(),
                    "optim": optim.state_dict(),
                }, ckpt_path
            )
        return ckpt_path

    def stop_fn(mean_rewards):
        return mean_rewards >= args.reward_threshold

    def train_fn(epoch, env_step):
        if env_step <= 10000:
            policy.set_eps(args.eps_train)
        elif env_step <= 50000:
            eps = args.eps_train - (env_step - 10000) / \
                40000 * (0.9 * args.eps_train)
            policy.set_eps(eps)
        else:
            policy.set_eps(0.1 * args.eps_train)

    def test_fn(epoch, env_step):
        policy.set_eps(args.eps_test)

    # trainer
    result = offpolicy_trainer(
        policy,
        train_collector,
        test_collector,
        args.epoch,
        args.step_per_epoch,
        args.step_per_collect,
        args.test_num,
        args.batch_size,
        update_per_step=args.update_per_step,
        train_fn=train_fn,
        test_fn=test_fn,
        stop_fn=stop_fn,
        save_best_fn=save_best_fn,
        logger=logger,
        save_checkpoint_fn=save_checkpoint_fn,
    )
    
    assert stop_fn(result['best_reward'])




if __name__ == '__main__':
    retrain_dqn()